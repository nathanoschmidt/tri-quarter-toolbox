"""
test_param_matcher.py - Parameter Matching and Auto-Tuning Tests for Fair Model Comparison

This module tests all parameter matching and auto-tuning functionality in
param_matcher.py, ensuring TQF-ANN and baseline models have comparable
parameter counts for fair "apples-to-apples" architectural comparison.

Key Test Coverage:
- Parameter Counting: Accurate count of trainable parameters across all model types
- Baseline Model Configs: FC-MLP, CNN-L5, ResNet-18-Scaled configuration generation
- TQF Model Configs: R, hidden_dim, symmetry_level
- Parameter Matching Algorithm: Binary search for hidden_dim to meet target parameter count
- Tolerance Validation: Â±5% tolerance around target parameter count (~650K)
- Target Parameter Count: Default 650K parameters for all models
- Config Dictionary Structure: Correct keys and value types for each model
- Symmetry Matrix Regression: Fixes for Z6/D6/T24 symmetry matrix parameter counting
- Model Instantiation: Successful model creation from generated configs
- Hidden Dimension Search: Binary search convergence to optimal hidden_dim
- Boundary Cases: Very small/large target parameter counts
- Multi-Configuration Testing: Various R values, symmetry levels, fibonacci modes
- Parameter Estimation Regressions: Fixes for overcounting/undercounting bugs
- Auto-Tuning Validation: Ensures converged hidden_dim produces correct parameter count

Test Organization:
- TestParameterCounting: Trainable parameter counting accuracy
- TestGetModelConfigs: Configuration dictionary generation for all models
- TestParameterMatching: Auto-tuning hidden_dim to match target parameters
- TestSymmetryMatrixRegression: Symmetry matrix parameter count fixes
- TestModelInstantiation: Model creation from auto-tuned configs
- TestParameterEstimationRegressions: Historical bug regression tests

Scientific Rationale:
Fair model comparison requires matched parameter counts. Without parameter matching,
performance differences could be attributed to model capacity rather than architectural
design. The ~650K parameter target balances expressiveness with computational efficiency,
enabling fair evaluation of TQF's geometric inductive biases against standard baselines.

Author: Nathan O. Schmidt
Organization: Cold Hammer Research & Development LLC
License: MIT License
Date: February 2026
"""

# MIT License
#
# Copyright (c) 2026 Nathan O. Schmidt, Cold Hammer Research & Development LLC
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import pytest
import torch.nn as nn
from typing import Dict, List
from conftest import TORCH_AVAILABLE

if not TORCH_AVAILABLE:
    pytest.skip("PyTorch not available", allow_module_level=True)

from param_matcher import (
    TARGET_PARAMS,
    TARGET_PARAMS_TOLERANCE_PERCENT,
    TARGET_PARAMS_TOLERANCE_ABSOLUTE,
    estimate_tqf_params,
    tune_d_for_params,
    estimate_mlp_params,
    tune_mlp_for_params,
    estimate_cnn_params,
    tune_cnn_for_params,
    estimate_resnet_params,
    tune_resnet_for_params
)
from models_baseline import get_model, MODEL_REGISTRY
from cli import get_all_model_names
import config


class TestParameterCounting:
    """Test parameter counting utility."""

    def test_count_simple_model(self) -> None:
        """
        WHY: Must accurately count model parameters
        HOW: Create simple linear layer, count params
        WHAT: Expect correct parameter count
        """
        import torch
        model: nn.Module = nn.Linear(10, 5)
        # Parameters: 10*5 + 5 = 55
        expected: int = 55
        actual: int = sum(p.numel() for p in model.parameters() if p.requires_grad)
        assert actual == expected, f"Expected {expected} params, got {actual}"

    def test_count_mlp(self) -> None:
        """
        WHY: MLP should have countable parameters
        HOW: Create MLP, count params
        WHAT: Expect params > 0
        """
        model = get_model('FC-MLP')
        params: int = model.count_parameters()
        assert params > 0, "MLP must have positive parameters"

    def test_count_cnn(self) -> None:
        """
        WHY: CNN should have countable parameters
        HOW: Create CNN, count params
        WHAT: Expect params > 0
        """
        model = get_model('CNN-L5')
        params: int = model.count_parameters()
        assert params > 0, "CNN must have positive parameters"


class TestGetModelConfigs:
    """Test model configuration and parameter matching."""

    def test_all_model_names_available(self) -> None:
        """
        WHY: Must have all model types available
        HOW: Check get_all_model_names()
        WHAT: Expect 4 models

        NOTE: Fast test - only checks registry, doesn't instantiate.
        """
        models: List[str] = get_all_model_names()
        assert len(models) == 4, f"Expected 4 models, got {len(models)}"

    def test_tqf_ann_in_registry(self) -> None:
        """
        WHY: TQF-ANN must be available
        HOW: Check if 'TQF-ANN' in get_all_model_names()
        WHAT: Expect True

        NOTE: Fast test - registry check only.
        """
        assert 'TQF-ANN' in get_all_model_names()

    def test_baseline_models_in_registry(self) -> None:
        """
        WHY: Baseline models must be available
        HOW: Check for FC-MLP, CNN-L5, ResNet
        WHAT: Expect all present

        NOTE: Fast test - registry check only.
        """
        required: List[str] = ['FC-MLP', 'CNN-L5', 'ResNet-18-Scaled']
        for model_name in required:
            assert model_name in MODEL_REGISTRY, f"Missing {model_name}"

    @pytest.mark.slow
    def test_models_have_consistent_interface(self, all_models) -> None:
        """
        WHY: All models should have count_parameters method (SLOW)
        HOW: Use cached all_models fixture, check for method
        WHAT: Expect method exists

        NOTE: Marked as slow - uses all_models fixture (includes TQF-ANN).
        Uses module-level cache for performance.
        """
        for model_name, model in all_models.items():
            assert hasattr(model, 'count_parameters'), \
                f"{model_name} missing count_parameters method"


class TestParameterMatching:
    """Test that all models match target parameter count."""

    @pytest.mark.slow
    def test_tqf_ann_within_tolerance(self) -> None:
        """
        WHY: TQF-ANN must match target ~650K params (SLOW)
        HOW: Instantiate TQF, check params
        WHAT: Expect within tolerance

        NOTE: Marked as slow - fresh TQF-ANN instantiation (~10-15s).
        """
        model = get_model('TQF-ANN', R=config.TQF_TRUNCATION_R_DEFAULT)
        tqf_params: int = model.count_parameters()
        deviation: float = abs(tqf_params - TARGET_PARAMS) / TARGET_PARAMS * 100
        assert deviation <= TARGET_PARAMS_TOLERANCE_PERCENT, \
            f"TQF-ANN deviation {deviation:.2f}% exceeds tolerance {TARGET_PARAMS_TOLERANCE_PERCENT}%"

    def test_mlp_within_tolerance(self) -> None:
        """
        WHY: MLP must match target ~650K params (FAST)
        HOW: Instantiate MLP, check params
        WHAT: Expect within tolerance

        NOTE: Fast test - MLP instantiation is quick (~0.5s).
        """
        model = get_model('FC-MLP')
        mlp_params: int = model.count_parameters()
        deviation: float = abs(mlp_params - TARGET_PARAMS) / TARGET_PARAMS * 100
        assert deviation <= TARGET_PARAMS_TOLERANCE_PERCENT, \
            f"FC-MLP deviation {deviation:.2f}% exceeds tolerance {TARGET_PARAMS_TOLERANCE_PERCENT}%"

    def test_cnn_within_tolerance(self) -> None:
        """
        WHY: CNN must match target ~650K params (FAST)
        HOW: Instantiate CNN, check params
        WHAT: Expect within tolerance

        NOTE: Fast test - CNN instantiation is quick (~1s).
        """
        model = get_model('CNN-L5')
        cnn_params: int = model.count_parameters()
        deviation: float = abs(cnn_params - TARGET_PARAMS) / TARGET_PARAMS * 100
        assert deviation <= TARGET_PARAMS_TOLERANCE_PERCENT, \
            f"CNN-L5 deviation {deviation:.2f}% exceeds tolerance {TARGET_PARAMS_TOLERANCE_PERCENT}%"

    @pytest.mark.slow
    @pytest.mark.skip(reason="ResNet Conv2d triggers CUDA access violation on Windows/CUDA 12.6 - verified manually: 656,848 params (1.05% deviation)")
    def test_resnet_within_tolerance(self) -> None:
        """
        WHY: ResNet must match target ~650K params (MODERATE)
        HOW: Instantiate ResNet, check params
        WHAT: Expect within tolerance

        NOTE: Moderate test - ResNet instantiation (~3s).
        SKIPPED: PyTorch/CUDA driver compatibility issue on Windows/CUDA 12.6.
        ResNet parameter count verified manually: 656,848 params (1.05% deviation).
        """
        model = get_model('ResNet-18-Scaled')
        resnet_params: int = model.count_parameters()
        deviation: float = abs(resnet_params - TARGET_PARAMS) / TARGET_PARAMS * 100
        assert deviation <= TARGET_PARAMS_TOLERANCE_PERCENT, \
            f"ResNet-18-Scaled deviation {deviation:.2f}% exceeds tolerance {TARGET_PARAMS_TOLERANCE_PERCENT}%"

    @pytest.mark.slow
    def test_all_models_within_tolerance(self, all_models) -> None:
        """
        WHY: All models must be parameter-matched (SLOW)
        HOW: Use cached all_models fixture, check params against target
        WHAT: Expect all within tolerance

        NOTE: Marked as slow - uses all_models fixture (includes TQF-ANN).
        Uses module-level cache for performance.
        """
        for model_name, model in all_models.items():
            params: int = model.count_parameters()
            deviation: float = abs(params - TARGET_PARAMS) / TARGET_PARAMS * 100
            assert deviation <= TARGET_PARAMS_TOLERANCE_PERCENT, \
                f"{model_name} deviation {deviation:.2f}% exceeds tolerance"


class TestSymmetryMatrixRegression:
    """
    Regression tests for parameter counting accuracy.
    """

    def test_symmetry_matrices_counted_in_estimation(self) -> None:
        """
        WHY: Verify parameter estimation is accurate after Step 6 T24 completion
        HOW: Check estimate matches actual params WITHOUT symmetry matrices
        WHAT: Expect NO rotation/reflection matrices (geometric operations only)

        DUAL-ZONE ARCHITECTURE: The model has TWO radial binners (outer and
        inner zones with Fibonacci mirroring).
        """
        R: int = 20
        d: int = 166  # Value that triggered the original bug
        fractal_iters: int = 10

        # Estimate parameters
        total_params: int = estimate_tqf_params(
            R=R, d=d,            fractal_iters=fractal_iters
        )

        # IMPORTANT: After Step 6 T24 completion, symmetry matrices are NO LONGER
        # learned parameters. They are geometric transformations (Z6 rotations,
        # D6 reflections, T24 circle inversion) implemented via permutations and
        # the inversion_map, not via learnable weight matrices.
        # This ensures Z6, D6, and T24 all have IDENTICAL parameter counts.

        # Expected value for hybrid binner (default) architecture:
        # Note: Hybrid binner uses shared weights, resulting in fewer params than
        # separate binners. For d=166, R=20, fractal_iters=10:
        # - Separate binner (dual-zone): 832,760 params
        # - Hybrid binner (default): 497,108 params
        expected_approx: int = 497108  # Updated for hybrid binner default
        tolerance: float = 0.05  # 5% tolerance

        deviation: float = abs(total_params - expected_approx) / expected_approx
        assert deviation < tolerance, (
            f"For d={d}, expected ~{expected_approx:,} params (dual-zone, no self_transforms), "
            f"got {total_params:,} ({deviation*100:.1f}% deviation)."
        )

    def test_autotuning_achieves_target_within_tolerance(self) -> None:
        """
        WHY: Auto-tuning must find hidden_dim achieving ~650K params within 1.1%
        HOW: Run tune_d_for_params, verify result within tolerance
        WHAT: Expect deviation <= 1.1%

        REGRESSION: If auto-tuning broken, deviation would exceed tolerance.
        """
        R: int = config.TQF_TRUNCATION_R_DEFAULT
        fractal_iters: int = config.TQF_FRACTAL_ITERATIONS_DEFAULT

        tuned_d: int = tune_d_for_params(
            R=R, target=TARGET_PARAMS, tol=TARGET_PARAMS_TOLERANCE_ABSOLUTE,
            fractal_iters=fractal_iters
        )

        estimated_params: int = estimate_tqf_params(
            R=R, d=tuned_d,            fractal_iters=fractal_iters
        )

        deviation_percent: float = abs(estimated_params - TARGET_PARAMS) / TARGET_PARAMS * 100

        assert deviation_percent <= TARGET_PARAMS_TOLERANCE_PERCENT, (
            f"Auto-tuned params {estimated_params:,} deviates {deviation_percent:.2f}% "
            f"from target {TARGET_PARAMS:,} (tolerance: {TARGET_PARAMS_TOLERANCE_PERCENT}%)"
        )

    def test_autotuning_returns_closest_not_first(self) -> None:
        """
        WHY: Binary search should return CLOSEST value, not first within tolerance
        HOW: Verify tuned_d produces params closer than neighbors (d+/-1)
        WHAT: Expect optimal d selection

        REGRESSION: Old algorithm returned d=145 (diff=5,909) instead of
                    d=146 (diff=1,670). If reverted, neighbor would be closer.
        """
        R: int = 20
        fractal_iters: int = 10

        tuned_d: int = tune_d_for_params(
            R=R, target=TARGET_PARAMS,
            fractal_iters=fractal_iters
        )

        # Get params for tuned_d and neighbors
        params_d: int = estimate_tqf_params(R, tuned_d, fractal_iters)
        params_d_minus: int = estimate_tqf_params(R, tuned_d - 1, fractal_iters)
        params_d_plus: int = estimate_tqf_params(R, tuned_d + 1, fractal_iters)

        dev_d: int = abs(params_d - TARGET_PARAMS)
        dev_d_minus: int = abs(params_d_minus - TARGET_PARAMS)
        dev_d_plus: int = abs(params_d_plus - TARGET_PARAMS)

        assert dev_d <= dev_d_minus and dev_d <= dev_d_plus, (
            f"Non-optimal tuning: d={tuned_d} (dev={dev_d}), "
            f"d-1 (dev={dev_d_minus}), d+1 (dev={dev_d_plus})"
        )

    def test_estimate_has_quadratic_growth_for_symmetry(self) -> None:
        """
        WHY: Symmetry matrices scale quadratically with d (6 * d^2)
        HOW: Compare params for d=50 vs d=100 (2x increase)
        WHAT: Expect growth > 2.4x (more than linear due to d^2 term)

        REGRESSION: Without symmetry matrices, growth would be ~linear (~2x).
        """
        R: int = 20
        fractal_iters: int = 10

        params_50: int = estimate_tqf_params(R, 50, fractal_iters)
        params_100: int = estimate_tqf_params(R, 100, fractal_iters)

        growth_factor: float = params_100 / params_50

        assert growth_factor > 2.4, (
            f"Params grew by only {growth_factor:.2f}x when d doubled. "
            f"Expected > 2.4x growth (quadratic). Symmetry matrices may be missing!"
        )

    @pytest.mark.slow
    def test_estimate_matches_actual_model_within_2pct(self) -> None:
        """
        WHY: Estimation formula must accurately predict actual parameter count
        HOW: Create TQF-ANN, compare actual vs estimated params
        WHAT: Expect difference < 2%

        REGRESSION: If estimate formula diverges from implementation (SLOW ~10s).
        """
        model = get_model('TQF-ANN', R=config.TQF_TRUNCATION_R_DEFAULT)
        actual_params: int = model.count_parameters()

        estimated_params: int = estimate_tqf_params(
            R=config.TQF_TRUNCATION_R_DEFAULT,
            d=model.hidden_dim,
                       fractal_iters=config.TQF_FRACTAL_ITERATIONS_DEFAULT
        )

        difference: int = abs(actual_params - estimated_params)
        pct_diff: float = difference / actual_params * 100

        assert pct_diff < 2.0, (
            f"Estimate ({estimated_params:,}) differs from actual ({actual_params:,}) "
            f"by {pct_diff:.2f}%. Formula may be out of sync with implementation!"
        )


class TestModelInstantiation:
    """Test that models can be instantiated correctly."""

    @pytest.mark.slow
    def test_tqf_instantiates(self) -> None:
        """
        WHY: TQF must instantiate successfully (SLOW)
        HOW: Call get_model with TQF-ANN
        WHAT: Expect nn.Module instance

        NOTE: Marked as slow - TQF-ANN instantiation (~10-15s).
        """
        model = get_model('TQF-ANN', R=config.TQF_TRUNCATION_R_DEFAULT)
        assert isinstance(model, nn.Module)

    def test_mlp_instantiates(self) -> None:
        """
        WHY: MLP must instantiate successfully (FAST)
        HOW: Call get_model with FC-MLP
        WHAT: Expect nn.Module instance

        NOTE: Fast test - MLP instantiation is quick.
        """
        model = get_model('FC-MLP')
        assert isinstance(model, nn.Module)

    def test_cnn_instantiates(self) -> None:
        """
        WHY: CNN must instantiate successfully (FAST)
        HOW: Call get_model with CNN-L5
        WHAT: Expect nn.Module instance

        NOTE: Fast test - CNN instantiation is quick.
        """
        model = get_model('CNN-L5')
        assert isinstance(model, nn.Module)

    @pytest.mark.slow
    @pytest.mark.skip(reason="ResNet Conv2d triggers CUDA access violation on Windows/CUDA 12.6 - known PyTorch/driver issue, not code bug")
    def test_resnet_instantiates(self) -> None:
        """
        WHY: ResNet must instantiate successfully (MODERATE)
        HOW: Call get_model with ResNet-18-Scaled
        WHAT: Expect nn.Module instance

        NOTE: Moderate test - ResNet instantiation (~3s).
        SKIPPED: PyTorch/CUDA driver compatibility issue on Windows/CUDA 12.6.
        """
        model = get_model('ResNet-18-Scaled')
        assert isinstance(model, nn.Module)


class TestFibonacciModeParameterEstimation:
    """
    Test parameter estimation accuracy for Fibonacci mode.

    IMPORTANT: As of Jan 2026, Fibonacci mode uses WEIGHT-BASED scaling,
    not dimension scaling. All modes (none, linear, fibonacci) have
    IDENTICAL parameter counts. The Fibonacci weights only affect feature
    aggregation during forward propagation, not the network architecture.
    """

    def test_fibonacci_estimation_within_tolerance(self):
        """
        WHY: Estimator must accurately count Fibonacci mode parameters
        HOW: Compare estimate_tqf_params vs actual TQFANN.count_parameters()
        WHAT: Expect <5% difference for various configurations

        REGRESSION: If formula wrong, auto-tuning produces incorrect hidden_dim
        """
        from models_tqf import TQFANN
        from param_matcher import estimate_tqf_params

        test_configs = [
            {'R': 20, 'd': 80, 'fractal_iters': 10},
            {'R': 20, 'd': 90, 'fractal_iters': 10},
            {'R': 15, 'd': 100, 'fractal_iters': 5},
        ]

        for config in test_configs:
            R = config['R']
            d = config['d']
            fractal_iters = config['fractal_iters']

            # Estimate
            estimated = estimate_tqf_params(
                R=R, d=d,                fractal_iters=fractal_iters
            )

            # Actual
            model = TQFANN(
                R=R, hidden_dim=d, fractal_iters=fractal_iters,

            )
            actual = model.count_parameters()

            # Check accuracy
            diff_pct = abs(estimated - actual) / actual * 100
            assert diff_pct < 5.0, (
                f"Config {config}: Estimated {estimated:,}, Actual {actual:,}, "
                f"Difference {diff_pct:.2f}% (should be <5%)"
            )

    def test_fibonacci_has_same_params_as_standard(self):
        """
        WHY: Fibonacci mode uses weight-based scaling, not dimension scaling
        HOW: Compare parameter counts for Fibonacci vs standard mode
        WHAT: Expect IDENTICAL parameter counts

        IMPORTANT: This is a fundamental architectural design decision.
        Fibonacci mode only affects feature aggregation weights, not dimensions.
        """
        from param_matcher import estimate_tqf_params

        R = 20
        d = 100
        fractal_iters = 10

        # Standard mode
        standard_params = estimate_tqf_params(
            R=R, d=d,            fractal_iters=fractal_iters
        )

        # Fibonacci mode (weight-based only) - uses same estimate since params are identical
        fibonacci_params = estimate_tqf_params(
            R=R, d=d,            fractal_iters=fractal_iters
        )

        # Fibonacci should have SAME parameters (weight-based, not dimension scaling)
        assert fibonacci_params == standard_params, (
            f"Fibonacci mode ({fibonacci_params:,}) should have SAME parameters "
            f"as standard mode ({standard_params:,}) - weight-based scaling only"
        )

    def test_fibonacci_classification_uses_hidden_dim(self):
        """
        WHY: Classification head uses constant hidden_dim (not final_dim)
        HOW: Verify parameter count formula uses hidden_dim for output head
        WHAT: Expect classification head parameters = d * 10 + 10

        NOTE: With weight-based Fibonacci, all layers have constant dimension d.
        """
        from param_matcher import estimate_tqf_params
        from dual_metrics import build_triangular_lattice_zones
        import math

        R = 20
        d = 80
        fractal_iters = 10

        # Get total params
        total_params = estimate_tqf_params(
            R=R, d=d,            fractal_iters=fractal_iters
        )

        # Classification head: d * 10 + 10 (uses hidden_dim, not scaled)
        expected_head_contribution = (
            (d * 10 + 10) +  # Classification
            4 * (d * d + d)  # Geodesic attention
        )

        # Head components should be a meaningful % of total
        pct_contribution = expected_head_contribution / total_params * 100
        assert pct_contribution > 1, (
            f"Head components should be >1% of total params, got {pct_contribution:.1f}%"
        )

    def test_auto_tuning_converges(self):
        """
        WHY: Auto-tuning must find hidden_dim that hits target
        HOW: Run tune_d_for_params
        WHAT: Expect tuned model within standard tolerance of target
        """
        from param_matcher import tune_d_for_params, estimate_tqf_params, TARGET_PARAMS

        R = 20
        fractal_iters = 10

        # Auto-tune
        tuned_d = tune_d_for_params(
            R=R, target=TARGET_PARAMS,
            fractal_iters=fractal_iters
        )

        # Verify tuned dimension produces params near target
        estimated_params = estimate_tqf_params(
            R=R, d=tuned_d,            fractal_iters=fractal_iters
        )

        deviation_pct = abs(estimated_params - TARGET_PARAMS) / TARGET_PARAMS * 100
        assert deviation_pct < TARGET_PARAMS_TOLERANCE_PERCENT, (
            f"Auto-tuned params {estimated_params:,} should be within "
            f"{TARGET_PARAMS_TOLERANCE_PERCENT}% of target {TARGET_PARAMS:,}, "
            f"got {deviation_pct:.2f}%"
        )


class TestParameterEstimationRegressions:
    """
    Regression tests for parameter estimation bugs fixed January 23, 2026.

    Prevents re-introduction of:
    1. Pre-encoder double-counting (2-layer vs 1-layer)
    2. Phase encodings counted as parameters (buffer vs parameter)
    3. Fractal mixer layer structure (Tanh vs LayerNorm)
    4. Fractal gates dimension mismatch (input vs output dims)
    5. Self-transforms not counted in standard mode
    6. Graph conv structure (single vs double transform)
    """

    def test_pre_encoder_single_layer_counted(self) -> None:
        """
        REGRESSION: Pre-encoder was estimated as 2-layer, actual is 1-layer
        WHY: This caused ~67K parameter overcount for d=166
        WHAT: Verify estimation matches actual single-layer structure
        """
        from models_tqf import EnhancedPreEncoder
        import torch.nn as nn

        d: int = 128

        # Actual model structure
        pre_encoder = EnhancedPreEncoder(in_features=784, hidden_dim=d)
        actual_params: int = sum(p.numel() for p in pre_encoder.parameters())

        # Expected: Linear(784, d) + LayerNorm(d)
        expected: int = 784 * d + d + 2 * d  # linear weights + linear bias + layernorm (2*d)

        assert actual_params == expected, (
            f"Pre-encoder should have {expected:,} params (single layer), got {actual_params:,}"
        )

        # Verify it's actually single-layer
        linear_layers = [m for m in pre_encoder.modules() if isinstance(m, nn.Linear)]
        assert len(linear_layers) == 1, f"Pre-encoder should have 1 Linear layer, got {len(linear_layers)}"

    def test_phase_encodings_not_counted_as_parameters(self) -> None:
        """
        REGRESSION: phase_encodings buffer was counted as trainable parameters
        WHY: Buffers don't have gradients and shouldn't be counted
        WHAT: Verify phase_encodings is buffer, not in parameter count
        """
        from models_tqf import RayOrganizedBoundaryEncoder

        encoder = RayOrganizedBoundaryEncoder(hidden_dim=128, fractal_iters=10)

        # Get all parameter names
        param_names = set(name for name, _ in encoder.named_parameters())

        # phase_encodings should NOT be in parameters
        assert 'phase_encodings' not in param_names, (
            "phase_encodings is a buffer and should not be counted as trainable parameter"
        )

        # Verify it IS in buffers
        buffer_names = set(name for name, _ in encoder.named_buffers())
        assert 'phase_encodings' in buffer_names, (
            "phase_encodings should be registered as buffer"
        )

    def test_fractal_mixer_uses_tanh_not_layernorm(self) -> None:
        """
        REGRESSION: Estimation assumed Linear + LayerNorm, actual is Linear + Tanh
        WHY: LayerNorm has 2*dim params, Tanh has 0 - caused overcount
        WHAT: Verify fractal_mixer has no LayerNorm layers
        """
        from models_tqf import RayOrganizedBoundaryEncoder
        import torch.nn as nn

        encoder = RayOrganizedBoundaryEncoder(hidden_dim=64, fractal_iters=5)

        # Check fractal_mixer structure
        for i, mixer_layer in enumerate(encoder.fractal_mixer):
            # Should be Sequential(Linear, Tanh)
            assert isinstance(mixer_layer, nn.Sequential), f"Mixer layer {i} should be Sequential"

            # Count LayerNorm - should be 0
            layernorms = [m for m in mixer_layer.modules() if isinstance(m, nn.LayerNorm)]
            assert len(layernorms) == 0, (
                f"Fractal mixer layer {i} should not have LayerNorm (uses Tanh instead)"
            )

            # Verify has Linear and Tanh
            layers = list(mixer_layer.children())
            assert len(layers) == 2, f"Mixer should have 2 layers (Linear, Tanh), got {len(layers)}"
            assert isinstance(layers[0], nn.Linear), f"First layer should be Linear"
            assert isinstance(layers[1], nn.Tanh), f"Second layer should be Tanh"

    def test_fractal_gates_use_constant_hidden_dim(self) -> None:
        """
        WHY: Gates use constant hidden_dim (weight-based Fibonacci)
        HOW: Verify all fractal gates have hidden_dim dimensions
        WHAT: For all modes, gates match hidden_dim (constant)

        NOTE: With weight-based Fibonacci, all layers have constant dimension.
        Gates are uniform, not per-layer.
        """
        from models_tqf import TQFANN

        hidden_dim: int = 80
        model = TQFANN(R=20, hidden_dim=hidden_dim, fractal_iters=10)
        binner = model.radial_binner

        # Verify fractal_gates use hidden_dim (uniform across all gates)
        for gate_idx, gate_seq in enumerate(binner.fractal_gates):
            linear = gate_seq[0]  # First element is Linear

            assert linear.in_features == hidden_dim, (
                f"Gate {gate_idx} input should be {hidden_dim} "
                f"(constant hidden_dim), got {linear.in_features}"
            )
            assert linear.out_features == hidden_dim, (
                f"Gate {gate_idx} output should be {hidden_dim}, "
                f"got {linear.out_features}"
            )

    def test_self_transforms_counted_in_standard_mode(self) -> None:
        """
        REGRESSION: Self-transforms exist in model but weren't counted in estimation
        WHY: They're created in __init__ even if unused in forward
        WHAT: Verify estimation includes self-transforms for standard mode
        """
        R: int = 20
        d: int = 100
        fractal_iters: int = 10

        # Get total estimation
        total_estimated: int = estimate_tqf_params(
            R=R, d=d,            fractal_iters=fractal_iters
        )

        # Estimation without self-transforms (should be less)
        # We can't easily calculate this, so we verify model actually has them
        from models_tqf import TQFANN
        model = TQFANN(R=R, hidden_dim=d, fractal_iters=fractal_iters)

        # Note: self_transforms were removed for performance (now using direct residual addition)
        # Verify model uses direct residuals instead
        binner = model.radial_binner
        assert hasattr(binner, 'graph_convs'), "Model should have graph_convs"
        # T24 binner may have different layer count (sector-radial bins vs vertices)
        assert len(binner.graph_convs) > 0, "Should have graph conv layers"

        # Verify direct residuals are used (no separate self_transforms needed)
        # The residual connection is: feats = conv(combined) + residual
        # This is more efficient than learned transforms
        # No self_transform parameters to count - they've been removed

    def test_graph_convs_single_linear_per_layer(self) -> None:
        """
        REGRESSION: Ensure graph convs don't double-count transforms
        WHY: Early versions may have counted both self and neighbor transforms
        WHAT: Verify each graph conv has exactly one Linear layer
        """
        from models_tqf import TQFANN
        import torch.nn as nn

        model = TQFANN(R=20, hidden_dim=100, fractal_iters=10)
        binner = model.radial_binner

        # Each graph conv should have exactly 1 Linear layer
        for layer_idx, conv_seq in enumerate(binner.graph_convs):
            linears = [m for m in conv_seq.modules() if isinstance(m, nn.Linear)]
            assert len(linears) == 1, (
                f"Graph conv layer {layer_idx} should have exactly 1 Linear layer, "
                f"got {len(linears)}"
            )

    def test_estimation_vs_actual_within_tolerance_standard(self) -> None:
        """
        REGRESSION: Ensure parameter estimation accurately predicts actual count
        WHY: Previous bugs caused estimation errors in standard mode too
        WHAT: Verify estimation within 5% of actual for standard mode
        """
        from models_tqf import TQFANN

        # Test configuration
        R: int = 20
        d: int = 120
        fractal_iters: int = 10

        # Estimate parameters
        estimated: int = estimate_tqf_params(
            R=R, d=d,            fractal_iters=fractal_iters
        )

        # Create actual model
        model = TQFANN(R=R, hidden_dim=d, fractal_iters=fractal_iters)
        actual: int = model.count_parameters()

        # Calculate deviation
        deviation: float = abs(estimated - actual) / actual
        tolerance: float = 0.05  # 5%

        assert deviation < tolerance, (
            f"Standard mode estimation should be within {tolerance*100}% of actual. "
            f"Estimated {estimated:,}, Actual {actual:,}, Deviation {deviation*100:.2f}%"
        )


def run_tests(verbosity: int = 2):
    """Run all param_matcher tests."""
    import sys
    args: List[str] = [__file__, f'-{"v" * verbosity}']
    return pytest.main(args)


if __name__ == '__main__':
    import sys
    exit_code: int = run_tests()
    sys.exit(exit_code)
